Epoch: [1]  [  0/145]  eta: 0:01:17  lr: 0.010000  loss: 1.4455 (1.4455)  loss_classifier: 0.6696 (0.6696)  loss_box_reg: 0.0063 (0.0063)  loss_objectness: 0.6941 (0.6941)  loss_rpn_box_reg: 0.0755 (0.0755)  time: 0.5355  data: 0.1821  max mem: 2617
/home/choi/anaconda3/envs/headhunter/lib/python3.8/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.
  warnings.warn("The default behavior for interpolate/upsample with float scale_factor changed "
iter_count: 10
iter_count: 20
iter_count: 30
iter_count: 40
iter_count: 50
iter_count: 60
iter_count: 70
iter_count: 80
iter_count: 90
iter_count: 100
iter_count: 110
iter_count: 120
iter_count: 130
iter_count: 140
Epoch: [1]  [144/145]  eta: 0:00:00  lr: 0.010000  loss: 1.1270 (1.0243)  loss_classifier: 0.1627 (0.1933)  loss_box_reg: 0.0517 (0.0249)  loss_objectness: 0.6332 (0.6613)  loss_rpn_box_reg: 0.0901 (0.1448)  time: 0.7481  data: 0.0070  max mem: 11869
Epoch: [1] Total time: 0:01:45 (0.7250 s / it)
Saving model
Epoch: [2]  [  0/145]  eta: 0:01:13  lr: 0.010000  loss: 0.9189 (0.9189)  loss_classifier: 0.2076 (0.2076)  loss_box_reg: 0.0489 (0.0489)  loss_objectness: 0.5977 (0.5977)  loss_rpn_box_reg: 0.0647 (0.0647)  time: 0.5065  data: 0.1669  max mem: 11869
iter_count: 150
iter_count: 160
iter_count: 170
iter_count: 180
iter_count: 190
iter_count: 200
iter_count: 210
iter_count: 220
iter_count: 230
iter_count: 240
iter_count: 250
iter_count: 260
iter_count: 270
iter_count: 280
iter_count: 290
Epoch: [2]  [144/145]  eta: 0:00:00  lr: 0.010000  loss: 1.2487 (1.0897)  loss_classifier: 0.4098 (0.3105)  loss_box_reg: 0.2704 (0.1889)  loss_objectness: 0.3911 (0.4563)  loss_rpn_box_reg: 0.0974 (0.1340)  time: 0.7510  data: 0.0077  max mem: 11869
Epoch: [2] Total time: 0:01:45 (0.7272 s / it)
Saving model
Epoch: [3]  [  0/145]  eta: 0:01:13  lr: 0.010000  loss: 1.2183 (1.2183)  loss_classifier: 0.4488 (0.4488)  loss_box_reg: 0.3785 (0.3785)  loss_objectness: 0.3251 (0.3251)  loss_rpn_box_reg: 0.0660 (0.0660)  time: 0.5037  data: 0.1676  max mem: 11869
iter_count: 300
iter_count: 310
iter_count: 320
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    train()
  File "train.py", line 184, in train
    train_one_epoch(model, optimizer, train_data_loader,
  File "/home/choi/hwang/workspace/HeadHunter/head_detection/vision/engine.py", line 65, in train_one_epoch
    losses.backward()
  File "/home/choi/anaconda3/envs/headhunter/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/choi/anaconda3/envs/headhunter/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
