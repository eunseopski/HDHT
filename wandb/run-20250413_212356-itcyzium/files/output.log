Epoch: [1]  [  0/145]  eta: 0:01:19  lr: 0.010000  loss: 1.4896 (1.4896)  loss_classifier: 0.7191 (0.7191)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.6937 (0.6937)  loss_rpn_box_reg: 0.0768 (0.0768)  time: 0.5513  data: 0.1920  max mem: 2617
/home/choi/anaconda3/envs/headhunter/lib/python3.8/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.
  warnings.warn("The default behavior for interpolate/upsample with float scale_factor changed "
Epoch: [1]  [144/145]  eta: 0:00:00  lr: 0.010000  loss: 1.0594 (1.0341)  loss_classifier: 0.1926 (0.1983)  loss_box_reg: 0.0478 (0.0310)  loss_objectness: 0.6277 (0.6594)  loss_rpn_box_reg: 0.0905 (0.1454)  time: 0.7558  data: 0.0068  max mem: 11869
Epoch: [1] Total time: 0:01:46 (0.7312 s / it)
Saving model
Epoch: [2]  [  0/145]  eta: 0:01:15  lr: 0.010000  loss: 1.0546 (1.0546)  loss_classifier: 0.2659 (0.2659)  loss_box_reg: 0.1278 (0.1278)  loss_objectness: 0.5926 (0.5926)  loss_rpn_box_reg: 0.0683 (0.0683)  time: 0.5217  data: 0.1751  max mem: 11869
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 10 that is less than the current step 140. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 20 that is less than the current step 140. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 30 that is less than the current step 140. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 40 that is less than the current step 140. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    train()
  File "train.py", line 184, in train
    train_one_epoch(model, optimizer, train_data_loader,
  File "/home/choi/hwang/workspace/HeadHunter/head_detection/vision/engine.py", line 65, in train_one_epoch
    losses.backward()
  File "/home/choi/anaconda3/envs/headhunter/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/choi/anaconda3/envs/headhunter/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
